# Spark-Based Data Ingestion Pipeline
Author: gardhaniissaac

Stack:
- Apache Airflow
- Apache Spark (local mode)
- PostgreSQL
- Docker & Docker Compose

## Project Structure
Project Structure
```.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ config
â”‚   â””â”€â”€ airflow.cfg
â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ configs
â”‚   â”‚   â”œâ”€â”€ assessments.yaml
â”‚   â”‚   â”œâ”€â”€ assessments_raw.yaml
â”‚   â”‚   â”œâ”€â”€ attendances.yaml
â”‚   â”‚   â”œâ”€â”€ attendances_raw.yaml
â”‚   â”‚   â”œâ”€â”€ daily_performances.yaml
â”‚   â”‚   â”œâ”€â”€ students.yaml
â”‚   â”‚   â””â”€â”€ students_raw.yaml
â”‚   â”œâ”€â”€ resources
â”‚   â”‚   â”œâ”€â”€ assessments.json
â”‚   â”‚   â”œâ”€â”€ attendances.csv
â”‚   â”‚   â””â”€â”€ students.csv
â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â”œâ”€â”€ tests
â”‚   â”‚   â”‚   â””â”€â”€ schema_loader_test.py
â”‚   â”‚   â”œâ”€â”€ db.py
â”‚   â”‚   â”œâ”€â”€ schema_loader.py
â”‚   â”‚   â””â”€â”€ spark_ingestion.py
â”‚   â”œâ”€â”€ assessments_dag.py
â”‚   â”œâ”€â”€ assessments_raw_dag.py
â”‚   â”œâ”€â”€ attendances_dag.py
â”‚   â”œâ”€â”€ attendances_raw_dag.py
â”‚   â”œâ”€â”€ daily_performance_dag.py
â”‚   â”œâ”€â”€ students_dag.py
â”‚   â””â”€â”€ students_raw_dag.py
â”œâ”€â”€ docker-compose.yaml
â””â”€â”€ requirements.txt
```
---

# ğŸš€ First Time Setup

## 1. Clone Repository

```bash
git clone <your-repo-url>
cd <your-repo-folder>
```

---

## 2. Create `.env` File

Create a file named `.env` in the project root:

```bash
touch .env
```

Generate secret key
```
openssl rand -hex 32
```

Add the following variables:

```env
# Airflow
AIRFLOW_UID=1000
AIRFLOW__WEBSERVER__SECRET_KEY=<generated-secret-key>
AIRFLOW__CORE__FERNET_KEY=$(python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")

# Postgres
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=airflow
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
```

Save the file.

---

## 3. Build Docker Images

```bash
docker compose build --no-cache
```

---

## 4. Initialize Airflow (Run Once)

```bash
docker compose up airflow-init
```

Wait until it finishes successfully.

---

## 5. Start All Services

```bash
docker compose up
```

---

# ğŸŒ Access Airflow

Open in browser:

```
http://localhost:8080
```

Login using:

```
Username: admin
Password: admin
```

Enable and trigger the DAG from the UI.

---

# ğŸ›‘ Stop Services

```bash
docker compose down
```

To completely reset (including database):

```bash
docker compose down -v
```

---

# ğŸ”„ Rebuild After Dependency Changes

If you modify:
- Dockerfile
- requirements.txt

Run:

```bash
docker compose down
docker compose build --no-cache
docker compose up
```